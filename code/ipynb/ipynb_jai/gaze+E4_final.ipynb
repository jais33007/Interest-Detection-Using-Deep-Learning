{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 18:14:22.003132: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/jai/miniconda3/envs/dl/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from keras.layers import LSTM, Dense, Conv1D, TimeDistributed, Flatten, Activation, Dropout, Bidirectional,concatenate\n",
    "from keras.callbacks import History, TensorBoard, Callback\n",
    "import keras.initializers as KI\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the gaze labelled data for each participant and document\n",
    "gaze_path = \"../../data/working/gaze_labelled/\"\n",
    "gaze_list = [pd.read_csv(data, index_col=0) for data in sorted(glob.glob(gaze_path + \"/*/*\"))]\n",
    "gaze_data = pd.concat(gaze_list, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_data.columns\n",
    "# features = ['left_gaze_x', 'left_gaze_y',\n",
    "#        'left_position_x', 'left_position_y', 'left_position_z', 'left_pupil',\n",
    "#        'right_gaze_x', 'right_gaze_y', 'right_position_x', 'right_position_y',\n",
    "#        'right_position_z', 'right_pupil','understand']\n",
    "\n",
    "features = ['left_gaze_x', 'left_gaze_y', 'right_gaze_x', 'right_gaze_y','understand','interest']\n",
    "\n",
    "participants = gaze_data.participant.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_data['interest'] = gaze_data['interest'].replace([1,2,3,4], [0,1,2,3])\n",
    "gaze_data['understand'] = gaze_data['understand'].replace([1,2,3,4], [0,1,2,3])\n",
    "#gaze_data['bin_interest'] = gaze_data['interest'].replace([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    mu = np.mean(dataset, axis=0)\n",
    "    sigma = np.std(dataset, axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "def get_frames(df, frame_size, step_size, label_name):\n",
    "\n",
    "    N_FEATURES = len(features) - 1\n",
    "\n",
    "    frames = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - frame_size, step_size):\n",
    "        for column in df.columns:\n",
    "            if column!=label_name:\n",
    "                column = df[column].values[i: i + frame_size]\n",
    "                frames.append([column])\n",
    "        else:\n",
    "            \n",
    "        # Retrieve the most often used label in this segment\n",
    "            label = stats.mode(df[label_name][i: i + frame_size])[0][0]\n",
    "            labels.append(label)\n",
    "\n",
    "    # Bring the segments into a better shape\n",
    "    frames = np.asarray(frames).reshape(-1, frame_size, N_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return frames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_freq = 60\n",
    "frame_size = samp_freq*30 \n",
    "step_size = samp_freq*15 \n",
    "\n",
    "def prepare_data(data, fold):\n",
    "  \n",
    "    df_val = data[data['participant'] == participants[fold]]\n",
    "    df_train = data[data['participant'] != participants[fold]]\n",
    "\n",
    "    # df_train.drop(['participant','document'], axis=1, inplace=True)\n",
    "    # df_val.drop(['participant','document'], axis=1, inplace=True)\n",
    "\n",
    "    df_train = df_train[features]\n",
    "    df_val = df_val[features]\n",
    "    \n",
    "    for col in df_train.columns:\n",
    "        \n",
    "        if col != 'interest':        \n",
    "            df_train[col] = normalize(df_train[col])\n",
    "            df_val[col] = normalize(df_val[col])\n",
    "\n",
    "    x_train, y_train = get_frames(df_train, frame_size, step_size, 'interest')\n",
    "    num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\n",
    "    num_classes = 4\n",
    "    print (x_train.shape, y_train.shape)\n",
    "\n",
    "    input_shape = (num_time_periods * num_sensors)\n",
    "    x_train = x_train.reshape(x_train.shape[0], input_shape)\n",
    "\n",
    "    # x_train = x_train.astype(\"float32\")\n",
    "    # y_train = y_train.astype(\"float32\")\n",
    "\n",
    "    #y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    x_val, y_val = get_frames(df_val, frame_size, step_size, 'interest')\n",
    "\n",
    "    num_time_periods1, num_sensors1 = x_val.shape[1], x_val.shape[2]\n",
    "\n",
    "    input_shape1 = (num_time_periods1 * num_sensors1)\n",
    "    x_val = x_val.reshape(x_val.shape[0], input_shape1)\n",
    "\n",
    "    # x_val = x_val.astype(\"float32\")\n",
    "    # y_val = y_val.astype(\"float32\")\n",
    "\n",
    "    #y_val = np_utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "    return num_time_periods, num_sensors, x_train, x_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_list_of_arrays = []\n",
    "scores = []\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "f1_per_fold = []\n",
    "prec_per_fold = []\n",
    "rec_per_fold = []\n",
    "\n",
    "def build_model(num_time_periods, num_sensors, num_classes, input_shape):\n",
    "  # 1D CNN neural network\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Reshape((frame_size, num_sensors), input_shape=(input_shape,)))\n",
    "    model.add(Conv1D(64, 5, activation='relu', input_shape=(frame_size, num_sensors)))\n",
    "    model.add(Conv1D(32, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(32, 10, activation='relu'))\n",
    "\n",
    "    model.add(Conv1D(15, 3, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(num_time_periods, num_sensors,t_x, val_x, t_y, val_y, EPOCHS, BATCH_SIZE):\n",
    "    \n",
    "    input_shape = (num_time_periods * num_sensors)\n",
    "    model = build_model(num_time_periods, num_sensors, 4, input_shape)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "    history = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, callbacks=[es], validation_split=0.3)\n",
    "    scores = model.evaluate(val_x, val_y)\n",
    "    print(f'Score : {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1])\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(val_x), axis=-1)\n",
    "    f1_per_fold.append(f1_score(val_y, y_pred, average='weighted'))\n",
    "    prec_per_fold.append(precision_score(val_y, y_pred, average='weighted'))\n",
    "    rec_per_fold.append(recall_score(val_y, y_pred, average='weighted'))\n",
    "    print('F1 score : ', f1_score(val_y, y_pred, average='weighted'))\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(val_y, y_pred, labels=[0,1,2,3])\n",
    "    print('Confusion Matrix : ')\n",
    "    print(cm)\n",
    "    \n",
    "    #conf_matrix_list_of_arrays.append(cm)\n",
    "    return history, scores, y_pred, acc_per_fold, f1_per_fold\n",
    "\n",
    "def plot_cmx(labels, predicteds, binary=False):\n",
    "    cmx = confusion_matrix(labels, predicteds)\n",
    "    cmx = cmx.astype('float') / cmx.sum(axis=1)[:, np.newaxis]\n",
    "    if binary:\n",
    "        class_names = ['not-interested', 'interested']\n",
    "    else:\n",
    "        class_names = ['1','2','3','4']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(\"mean accuracy: %.2f\" % accuracy_score(labels, predicteds))\n",
    "    sns.heatmap(cmx, annot=True, fmt=\".0%\",center=1, xticklabels=class_names, yticklabels=class_names)\n",
    "\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(i):\n",
    "    return pd.read_csv(i, index_col=0)\n",
    "\n",
    "data_path = \"../../data/output/E4_mapped/\"\n",
    "output_path = \"../../data/output/E4_comb_mods/\"\n",
    "participants = [\"p\" + str(i).zfill(2) for i in range(1, 16) if i != 3 and i != 4]\n",
    "\n",
    "def participant_data(data):\n",
    "    \n",
    "    for p in participants:\n",
    "        files = [f for f in sorted(glob.glob(data_path + str(p) + \"/*\" + str(data) + \".csv\"))]\n",
    "        p_data = pd.concat(map(f, files), ignore_index=True)\n",
    "        #p_data.to_csv(output_path + str(p) + \"_\" + str(data) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = [\"ACC\", \"EDA\", \"TEMP\", \"BVP\", \"HR\"]\n",
    "for m in modalities:\n",
    "    participant_data(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(samples):\n",
    "    std = np.std(samples)\n",
    "    if std == 0:\n",
    "        return samples - np.mean(samples)\n",
    "    else:\n",
    "        return (samples - np.mean(samples)) / std\n",
    "\n",
    "acc_freq = 32\n",
    "eda_freq = 4\n",
    "temp_freq = 4\n",
    "bvp_freq = 64\n",
    "hr_freq = 1\n",
    "\n",
    "#sliding window - set window size and step size \n",
    "def create_window(pid, win_size=30, win_step=15, Binary=False):\n",
    "    \n",
    "    for f in os.listdir(output_path):\n",
    "        if f.startswith(str(pid)) and f.endswith(\"ACC.csv\"):\n",
    "            acc_path = f\n",
    "        elif f.startswith(str(pid)) and f.endswith(\"EDA.csv\"):\n",
    "            eda_path = f\n",
    "        elif f.startswith(str(pid)) and f.endswith(\"TEMP.csv\"):\n",
    "            temp_path = f\n",
    "        elif f.startswith(str(pid)) and f.endswith(\"BVP.csv\"):\n",
    "            bvp_path = f\n",
    "        elif f.startswith(str(pid)) and f.endswith(\"HR.csv\"):\n",
    "            hr_path = f\n",
    "    \n",
    "    acc_data = pd.read_csv(output_path + acc_path, index_col=0 )\n",
    "    eda_data = pd.read_csv(output_path + eda_path, index_col=0)\n",
    "    temp_data = pd.read_csv(output_path + temp_path, index_col=0)\n",
    "    bvp_data = pd.read_csv(output_path + bvp_path, index_col=0)\n",
    "    #hr_data = pd.read_csv(output_path + hr_path, index_col=0)\n",
    "\n",
    "    eda_data['interest'] = eda_data['interest'].replace([1,2,3,4], [0,1,2,3])\n",
    "    bvp_data['understand'] = bvp_data['understand'].replace([1,2,3,4], [0,1,2,3]) \n",
    "    \n",
    "    acc_x = []; acc_y = []; acc_z = []; eda = []; temp=[]; label=[]; bvp=[]; hr=[]; ut=[]; win_size = 30 ; win_step= 15; \n",
    "    total_time = int(len(eda_data) / eda_freq)\n",
    "\n",
    "    for i in range(win_size, total_time, win_step):\n",
    "\n",
    "        acc_x.append(s(acc_data[\"acc_x\"][acc_freq * (i - win_size): acc_freq * i]))\n",
    "        acc_y.append(s(acc_data[\"acc_y\"][acc_freq * (i - win_size): acc_freq * i]))\n",
    "        acc_z.append(s(acc_data[\"acc_z\"][acc_freq * (i - win_size): acc_freq * i]))\n",
    "\n",
    "        eda.append(s(eda_data[\"data\"][eda_freq * (i - win_size): eda_freq * i]))\n",
    "        temp.append(s(temp_data[\"data\"][temp_freq * (i - win_size): temp_freq * i]))\n",
    "        bvp.append(s(bvp_data[\"data\"][bvp_freq * (i - win_size): bvp_freq * i]))\n",
    "        #hr.append(s(hr_data[\"data\"][hr_freq * (i - win_size): hr_freq * i]))\n",
    "        ut.append(bvp_data[\"understand\"][bvp_freq * (i - win_size): bvp_freq * i])\n",
    "\n",
    "        l = stats.mode(eda_data[\"interest\"][eda_freq * (i - win_size): eda_freq * i])[0][0]\n",
    "        if Binary:\n",
    "            if(l == 0) | (l == 1):\n",
    "                l = 0\n",
    "                label.append(l)\n",
    "            else:\n",
    "                l = 1\n",
    "                label.append(l)\n",
    "        else:\n",
    "\n",
    "        #l_vote = np.bincount(np.array(l)).argmax()\n",
    "            label.append(l)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"label\": np.array(label),\n",
    "        \"acc_x\": np.array(acc_x), \n",
    "        \"acc_y\": np.array(acc_y), \n",
    "        \"acc_z\": np.array(acc_z),\n",
    "        \"eda\": np.array(eda),\n",
    "         \"bvp\": np.array(bvp), \n",
    "         \"temp\": np.array(temp),\n",
    "         \"ut\": np.array(ut)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = [] \n",
    "cvscores = []\n",
    "predicted = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(participants)):\n",
    "    \n",
    "    pred = []\n",
    "    label = []\n",
    "    print(\"Training with participant \"+ participants[i] +\" left out\")\n",
    "    num_time_periods, num_sensors, t_x, val_x, t_y, val_y = prepare_data(gaze_data, i)\n",
    "    results, scores, preds, acc, f1 = evaluate(num_time_periods, num_sensors,t_x, val_x, t_y, val_y, EPOCHS, BATCH_SIZE)\n",
    "    model_history.append(results)\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    predicted.extend(preds)\n",
    "    labels.extend(val_y) \n",
    "    print(\"Accuracy with participant \" + participants[i] + ' as test :'+str(scores[1]*100))\n",
    "    print(\"=======\"*12, end=\"\\n\\n\\n\")\n",
    "\n",
    "#predicted = np.argmax(predicted, axis=1)\n",
    "plot_cmx(labels, predicted)\n",
    "print ('Average accuracy with all the participants')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
